Fine-tuned Google's flan-t5 on a large-scale dialogue summarization dataset using techniques like LoRA and parameter-efficient fine-tuning, reducing inference errors by 20%.


Developed a pre-processing pipeline to clean, tokenize, and balance the dataset, improving model training efficiency and summarization accuracy. 
